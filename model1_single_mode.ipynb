{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os, gc\n",
    "import zarr\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "from collections import Counter\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#level5 toolkit\n",
    "#We are required to use L5Kit toolkit provided by the competition host \n",
    "#to prepare/preprocess data, trian and evaluate the mode\n",
    "from l5kit.data import PERCEPTION_LABELS\n",
    "from l5kit.dataset import EgoDataset, AgentDataset\n",
    "from l5kit.data import ChunkedDataset, LocalDataManager\n",
    "\n",
    "# level5 toolkit \n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.geometry import transform_points\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.visualization import draw_trajectory, draw_reference_trajectory, TARGET_POINTS_COLOR\n",
    "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\n",
    "#L5Kit is a library which lets you:Â¶\n",
    "#Load driving scenes from zarr files\n",
    "#Read semantic maps\n",
    "#Read aerial maps\n",
    "#Create birds-eye-view (BEV) images which represent a scene around an AV or another vehicle\n",
    "#Sample data\n",
    "#Train neural networks\n",
    "#Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from colorama import Fore, Back, Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.resnet import resnet18, resnet50, resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbatch size\\nThe batch size defines the number of samples that will be propagated through the network.\\n\\nFor instance, let\\'s say you have 1050 training samples and you want to set up a batch_size equal to 100.\\nThe algorithm takes the first 100 samples (from 1st to 100th) from the training dataset and trains the network. \\nNext, it takes the second 100 samples (from 101st to 200th) and trains the network again.\\nWe can keep doing this procedure until we have propagated all samples through of the network.\\nProblem might happen with the last set of samples.\\nIn our example, we\\'ve used 1050 which is not divisible by 100 without remainder. \\nThe simplest solution is just to get the final 50 samples and train the network.\\n\\nAdvantages of using a batch size < number of all samples:\\n\\nIt requires less memory. Since you train the network using fewer samples, \\nthe overall training procedure requires less memory. \\nThat\\'s especially important if you are not able to fit the whole dataset in your machine\\'s memory.\\n\\nTypically networks train faster with mini-batches. \\nThat\\'s because we update the weights after each propagation. \\nIn our example we\\'ve propagated 11 batches (10 of them had 100 samples and 1 had 50 samples) \\nand after each of them we\\'ve updated our network\\'s parameters. \\nIf we used all samples during propagation we would make only 1 update for the network\\'s parameter.\\n\\nDisadvantages of using a batch size < number of all samples:\\n\\nThe smaller the batch the less accurate the estimate of the gradient will be. \\nIn the figure below, you can see that the direction of the mini-batch gradient (green color)\\nfluctuates much more in comparison to the direction of the full batch gradient (blue color).\\n\\nshuffle\\nShuffling data serves the purpose of reducing variance and making sure that models remain general and overfit less.\\n\\nThe obvious case where you\\'d shuffle your data is if your data is sorted by their class/target. \\nHere, you will want to shuffle to make sure that your training/test/validation sets are representative of \\nthe overall distribution of the data.\\n\\nFor batch gradient descent, the same logic applies. The idea behind batch gradient descent is \\nthat by calculating the gradient on a single batch, you will usually get a fairly good estimate \\nof the \"true\" gradient. That way, you save computation time by not having to calculate the \"true\"\\ngradient over the entire dataset every time.\\n\\nYou want to shuffle your data after each epoch because you will always have the risk \\nto create batches that are not representative of the overall dataset, and therefore, \\nyour estimate of the gradient will be off. Shuffling your data after each epoch ensures \\nthat you will not be \"stuck\" with too many bad batches.\\n\\nIn regular stochastic gradient descent, when each batch has size 1, you still want to shuffle your data \\nafter each epoch to keep your learning general. Indeed, if data point 17 is always used after data point 16, \\nits own gradient will be biased with whatever updates data point 16 is making on the model. By shuffling your data,\\nyou ensure that each data point creates an \"independent\" change on the model, without being biased\\nby the same points before them.\\n\\nnum_worker\\nTo speed up the training process, we will make use of the num_workers optional attribute of the DataLoader class.\\n\\nThe num_workers attribute tells the data loader instance how many sub-processes to use for data loading.\\nBy default, the num_workers value is set to zero, and a value of zero tells the loader to load the data \\ninside the main process.\\n\\nThis means that the training process will work sequentially inside the main process. \\nAfter a batch is used during the training process and another one is needed, we read the batch data from disk.\\n\\nNow, if we have a worker process, we can make use of the fact that our machine has multiple cores. \\nThis means that the next batch can already be loaded and ready to go by the time the main process \\nis ready for another batch. \\nThis is where the speed up comes from. The batches are loaded using additional worker processes \\nand are queued up in memory.\\n\\nresnet \\nDeeper neural networks are more difficult to train. Why? One big problem of a deep network is\\nthe vanishing gradient problem. Basically, the deeper the harder to train.\\nTo solve this problem, proposed to use a reference to the previous layer to \\ncompute the output at a given layer. In ResNet, the output from the previous layer, called residual,\\nis added to the output of the current layer. \\n\\nA residual network, or ResNet for short, is an artificial neural network that helps \\nto build deeper neural network by utilizing skip connections or shortcuts to jump over some layers.\\nskipping helps build deeper network layers without falling into the problem of vanishing gradients.\\nThere are different versions of ResNet, including ResNet-18, ResNet-34, ResNet-50, \\nand so on. The numbers denote layers, although the architecture is the same.\\n\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "batch size\n",
    "The batch size defines the number of samples that will be propagated through the network.\n",
    "\n",
    "For instance, let's say you have 1050 training samples and you want to set up a batch_size equal to 100.\n",
    "The algorithm takes the first 100 samples (from 1st to 100th) from the training dataset and trains the network. \n",
    "Next, it takes the second 100 samples (from 101st to 200th) and trains the network again.\n",
    "We can keep doing this procedure until we have propagated all samples through of the network.\n",
    "Problem might happen with the last set of samples.\n",
    "In our example, we've used 1050 which is not divisible by 100 without remainder. \n",
    "The simplest solution is just to get the final 50 samples and train the network.\n",
    "\n",
    "Advantages of using a batch size < number of all samples:\n",
    "\n",
    "It requires less memory. Since you train the network using fewer samples, \n",
    "the overall training procedure requires less memory. \n",
    "That's especially important if you are not able to fit the whole dataset in your machine's memory.\n",
    "\n",
    "Typically networks train faster with mini-batches. \n",
    "That's because we update the weights after each propagation. \n",
    "In our example we've propagated 11 batches (10 of them had 100 samples and 1 had 50 samples) \n",
    "and after each of them we've updated our network's parameters. \n",
    "If we used all samples during propagation we would make only 1 update for the network's parameter.\n",
    "\n",
    "Disadvantages of using a batch size < number of all samples:\n",
    "\n",
    "The smaller the batch the less accurate the estimate of the gradient will be. \n",
    "In the figure below, you can see that the direction of the mini-batch gradient (green color)\n",
    "fluctuates much more in comparison to the direction of the full batch gradient (blue color).\n",
    "\n",
    "shuffle\n",
    "Shuffling data serves the purpose of reducing variance and making sure that models remain general and overfit less.\n",
    "\n",
    "The obvious case where you'd shuffle your data is if your data is sorted by their class/target. \n",
    "Here, you will want to shuffle to make sure that your training/test/validation sets are representative of \n",
    "the overall distribution of the data.\n",
    "\n",
    "For batch gradient descent, the same logic applies. The idea behind batch gradient descent is \n",
    "that by calculating the gradient on a single batch, you will usually get a fairly good estimate \n",
    "of the \"true\" gradient. That way, you save computation time by not having to calculate the \"true\"\n",
    "gradient over the entire dataset every time.\n",
    "\n",
    "You want to shuffle your data after each epoch because you will always have the risk \n",
    "to create batches that are not representative of the overall dataset, and therefore, \n",
    "your estimate of the gradient will be off. Shuffling your data after each epoch ensures \n",
    "that you will not be \"stuck\" with too many bad batches.\n",
    "\n",
    "In regular stochastic gradient descent, when each batch has size 1, you still want to shuffle your data \n",
    "after each epoch to keep your learning general. Indeed, if data point 17 is always used after data point 16, \n",
    "its own gradient will be biased with whatever updates data point 16 is making on the model. By shuffling your data,\n",
    "you ensure that each data point creates an \"independent\" change on the model, without being biased\n",
    "by the same points before them.\n",
    "\n",
    "num_worker\n",
    "To speed up the training process, we will make use of the num_workers optional attribute of the DataLoader class.\n",
    "\n",
    "The num_workers attribute tells the data loader instance how many sub-processes to use for data loading.\n",
    "By default, the num_workers value is set to zero, and a value of zero tells the loader to load the data \n",
    "inside the main process.\n",
    "\n",
    "This means that the training process will work sequentially inside the main process. \n",
    "After a batch is used during the training process and another one is needed, we read the batch data from disk.\n",
    "\n",
    "Now, if we have a worker process, we can make use of the fact that our machine has multiple cores. \n",
    "This means that the next batch can already be loaded and ready to go by the time the main process \n",
    "is ready for another batch. \n",
    "This is where the speed up comes from. The batches are loaded using additional worker processes \n",
    "and are queued up in memory.\n",
    "\n",
    "resnet \n",
    "Deeper neural networks are more difficult to train. Why? One big problem of a deep network is\n",
    "the vanishing gradient problem. Basically, the deeper the harder to train.\n",
    "To solve this problem, proposed to use a reference to the previous layer to \n",
    "compute the output at a given layer. In ResNet, the output from the previous layer, called residual,\n",
    "is added to the output of the current layer. \n",
    "\n",
    "A residual network, or ResNet for short, is an artificial neural network that helps \n",
    "to build deeper neural network by utilizing skip connections or shortcuts to jump over some layers.\n",
    "skipping helps build deeper network layers without falling into the problem of vanishing gradients.\n",
    "There are different versions of ResNet, including ResNet-18, ResNet-34, ResNet-50, \n",
    "and so on. The numbers denote layers, although the architecture is the same.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "# training cfg\n",
    "training_cfg = {\n",
    "    \n",
    "    'format_version': 4,\n",
    "    \n",
    "     ## Model options\n",
    "    'model_params': {\n",
    "        'model_architecture': 'resnet34',\n",
    "        'history_num_frames': 10,\n",
    "        'history_step_size': 1,\n",
    "        'history_delta_time': 0.1,\n",
    "        'future_num_frames': 50,\n",
    "        'future_step_size': 1,\n",
    "        'future_delta_time': 0.1,\n",
    "    },\n",
    "\n",
    "    ## Input raster parameters\n",
    "    'raster_params': {\n",
    "        \n",
    "        'raster_size': [300,300], # raster's spatial resolution [meters per pixel]: \n",
    "        #the size in the real world one pixel corresponds to.\n",
    "        # set by yourself: 300*300, 350*350, 450*450, 600*600\n",
    "        'pixel_size': [0.5, 0.5], # From 0 to 1 per axis, [0.5,0.5] would show the ego centered in the image.\n",
    "        'ego_center': [0.25, 0.5],\n",
    "        'map_type': \"py_semantic\",\n",
    "        \n",
    "        # the keys are relative to the dataset environment variable\n",
    "        'satellite_map_key': \"aerial_map/aerial_map.png\",\n",
    "        'semantic_map_key': \"semantic_map/semantic_map.pb\",\n",
    "        'dataset_meta_key': \"meta.json\",\n",
    "\n",
    "        # e.g. 0.0 include every obstacle, 0.5 show those obstacles with >0.5 probability of being\n",
    "        # one of the classes we care about (cars, bikes, peds, etc.), >=1.0 filter all other agents.\n",
    "        'filter_agents_threshold': 0.5\n",
    "    },\n",
    "\n",
    "    ## Data loader options\n",
    "    'train_data_loader': {\n",
    "        'key': \"scenes/train.zarr\",\n",
    "        'batch_size': 16, # set by yourself,8, 16,32\n",
    "        'shuffle': True,\n",
    "        'num_workers': 4\n",
    "    },\n",
    "\n",
    "    ## Train params\n",
    "    'train_params': {\n",
    "        'checkpoint_every_n_steps': 5000,\n",
    "        'max_num_steps': 100 if DEBUG else 20000 #set by yourself, 10000, 20000, 25000, 30000,  75000\n",
    "    }\n",
    "}\n",
    "\n",
    "# inference cfg\n",
    "inference_cfg = {\n",
    "    \n",
    "    'format_version': 4,\n",
    "    'model_params': {\n",
    "        'history_num_frames': 10,\n",
    "        'history_step_size': 1,\n",
    "        'history_delta_time': 0.1,\n",
    "        'future_num_frames': 50,\n",
    "        'future_step_size': 1,\n",
    "        'future_delta_time': 0.1\n",
    "    },\n",
    "    \n",
    "    'raster_params': {\n",
    "        'raster_size': [300, 300], # same as train\n",
    "        'pixel_size': [0.5, 0.5],\n",
    "        'ego_center': [0.25, 0.5],\n",
    "        'map_type': 'py_semantic',\n",
    "        'satellite_map_key': 'aerial_map/aerial_map.png',\n",
    "        'semantic_map_key': 'semantic_map/semantic_map.pb',\n",
    "        'dataset_meta_key': 'meta.json',\n",
    "        'filter_agents_threshold': 0.5\n",
    "    },\n",
    "    \n",
    "        'test_data_loader': {\n",
    "        'key': 'scenes/test.zarr',\n",
    "        'batch_size': 16, \n",
    "        'shuffle': False,\n",
    "        'num_workers': 4\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Couple of things to note:\n",
    "\n",
    "#model_architecture: you can put 'resnet18', 'resnet34' or 'resnet50'.\n",
    "#raster_size: specify the size of the image, the default is [224,224].\n",
    "#Increase raster_size can improve the score. However the training time will be significantly longer.\n",
    "#batch_size: number of inputs for one forward pass, again one of the parameters to tune.\n",
    "#max_num_steps: the number of iterations to train, i.e. number of epochs.\n",
    "#checkpoint_every_n_steps: the model will be saved at every n steps,\n",
    "#again change this number as to how you want to keep track of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'format_version': 4, 'model_params': {'model_architecture': 'resnet34', 'history_num_frames': 10, 'history_step_size': 1, 'history_delta_time': 0.1, 'future_num_frames': 50, 'future_step_size': 1, 'future_delta_time': 0.1}, 'raster_params': {'raster_size': [300, 300], 'pixel_size': [0.5, 0.5], 'ego_center': [0.25, 0.5], 'map_type': 'py_semantic', 'satellite_map_key': 'aerial_map/aerial_map.png', 'semantic_map_key': 'semantic_map/semantic_map.pb', 'dataset_meta_key': 'meta.json', 'filter_agents_threshold': 0.5}, 'train_data_loader': {'key': 'scenes/train.zarr', 'batch_size': 16, 'shuffle': True, 'num_workers': 4}, 'train_params': {'checkpoint_every_n_steps': 5000, 'max_num_steps': 20000}}\n"
     ]
    }
   ],
   "source": [
    "# root directory\n",
    "DIR_INPUT = \"/Users/shuozhang/Downloads/lyft-motion-prediction-autonomous-vehicles/\"\n",
    "\n",
    "#submission\n",
    "SINGLE_MODE_SUBMISSION = f\"{DIR_INPUT}/single_mode_sample_submission.csv\"\n",
    "MULTI_MODE_SUBMISSION = f\"{DIR_INPUT}/multi_mode_sample_submission.csv\"\n",
    "\n",
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = DIR_INPUT\n",
    "dm = LocalDataManager(None)\n",
    "print(training_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "| Num Scenes | Num Frames | Num Agents | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
      "+------------+------------+------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "|   16265    |  4039527   | 320124624  |      112.19     |        248.36        |        79.25         |        24.83         |        10.00        |\n",
      "+------------+------------+------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "# training cfg\n",
    "train_cfg = training_cfg[\"train_data_loader\"]\n",
    "\n",
    "# rasterizer\n",
    "rasterizer = build_rasterizer(training_cfg, dm)\n",
    "\n",
    "# dataloader\n",
    "train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\n",
    "train_dataset = AgentDataset(training_cfg, train_zarr, rasterizer)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=train_cfg[\"shuffle\"], batch_size=train_cfg[\"batch_size\"], \n",
    "                             num_workers=train_cfg[\"num_workers\"])\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyftModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone = resnet34(pretrained=True) \n",
    "        \n",
    "        num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
    "        num_in_channels = 3 + num_history_channels\n",
    "\n",
    "        self.backbone.conv1 = nn.Conv2d(\n",
    "            num_in_channels,\n",
    "            self.backbone.conv1.out_channels,\n",
    "            kernel_size=self.backbone.conv1.kernel_size,\n",
    "            stride=self.backbone.conv1.stride,\n",
    "            padding=self.backbone.conv1.padding,\n",
    "            bias=False,\n",
    "        )\n",
    "        \n",
    "        # This is 512 for resnet18 and resnet34;\n",
    "        # And it is 2048 for the other resnets\n",
    "        backbone_out_features = 512\n",
    "        \n",
    "        # X, Y coords for the future positions \n",
    "        num_targets = 2 * cfg[\"model_params\"][\"future_num_frames\"]\n",
    "\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_features=backbone_out_features, out_features=4096),\n",
    "        )\n",
    "\n",
    "        self.logit = nn.Linear(4096, out_features=num_targets)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "\n",
    "        x = self.backbone.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.head(x)\n",
    "        x = self.logit(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLyftMultiModel(\\n  (backbone): ResNet(\\n    (conv1): Conv2d(25, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n    (relu): ReLU(inplace=True)\\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\\n    (layer1): Sequential(\\n      (0): BasicBlock(\\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n        (relu): ReLU(inplace=True)\\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n      )\\n      (1): BasicBlock(\\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n        (relu): ReLU(inplace=True)\\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n      )\\n    )\\n    (layer2): Sequential(\\n      (0): BasicBlock(\\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n        (relu): ReLU(inplace=True)\\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n        (downsample): Sequential(\\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n        )\\n      )\\n      (1): BasicBlock(\\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n        (relu): ReLU(inplace=True)\\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n      )\\n    )\\n    (layer3): Sequential(\\n      (0): BasicBlock(\\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n        (relu): ReLU(inplace=True)\\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n        (downsample): Sequential(\\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n        )\\n      )\\n      (1): BasicBlock(\\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n        (relu): ReLU(inplace=True)\\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n      )\\n    )\\n    (layer4): Sequential(\\n      (0): BasicBlock(\\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n        (relu): ReLU(inplace=True)\\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n        (downsample): Sequential(\\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n        )\\n      )\\n      (1): BasicBlock(\\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n        (relu): ReLU(inplace=True)\\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n      )\\n    )\\n    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\\n    (fc): Linear(in_features=512, out_features=1000, bias=True)\\n  )\\n  (head): Sequential(\\n    (0): Linear(in_features=512, out_features=4096, bias=True)\\n  )\\n  (logit): Linear(in_features=4096, out_features=303, bias=True)\\n)\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "LyftMultiModel(\n",
    "  (backbone): ResNet(\n",
    "    (conv1): Conv2d(25, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (relu): ReLU(inplace=True)\n",
    "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
    "    (layer1): Sequential(\n",
    "      (0): BasicBlock(\n",
    "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (relu): ReLU(inplace=True)\n",
    "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      )\n",
    "      (1): BasicBlock(\n",
    "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (relu): ReLU(inplace=True)\n",
    "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      )\n",
    "    )\n",
    "    (layer2): Sequential(\n",
    "      (0): BasicBlock(\n",
    "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (relu): ReLU(inplace=True)\n",
    "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (downsample): Sequential(\n",
    "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "      )\n",
    "      (1): BasicBlock(\n",
    "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (relu): ReLU(inplace=True)\n",
    "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      )\n",
    "    )\n",
    "    (layer3): Sequential(\n",
    "      (0): BasicBlock(\n",
    "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (relu): ReLU(inplace=True)\n",
    "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (downsample): Sequential(\n",
    "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "      )\n",
    "      (1): BasicBlock(\n",
    "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (relu): ReLU(inplace=True)\n",
    "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      )\n",
    "    )\n",
    "    (layer4): Sequential(\n",
    "      (0): BasicBlock(\n",
    "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (relu): ReLU(inplace=True)\n",
    "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (downsample): Sequential(\n",
    "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "      )\n",
    "      (1): BasicBlock(\n",
    "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (relu): ReLU(inplace=True)\n",
    "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      )\n",
    "    )\n",
    "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
    "  )\n",
    "  (head): Sequential(\n",
    "    (0): Linear(in_features=512, out_features=4096, bias=True)\n",
    "  )\n",
    "  (logit): Linear(in_features=4096, out_features=303, bias=True)\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LyftModel(training_cfg).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get hardware type (CPU, GPU, TPU)\n",
    "device # cuba:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 13.90205192565918 loss(avg): 123.70076433817546:   0%|          | 3/20000 [00:36<70:23:04, 12.67s/it] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-63be2e5aaf9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "tr_it = iter(train_dataloader)\n",
    "progress_bar = tqdm(range(training_cfg[\"train_params\"][\"max_num_steps\"]))\n",
    "\n",
    "losses_train = []\n",
    "\n",
    "for _ in progress_bar:\n",
    "    try:\n",
    "        data = next(tr_it)\n",
    "    except StopIteration:\n",
    "        tr_it = iter(train_dataloader)\n",
    "        data = next(tr_it)\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    # forward pass\n",
    "    inputs = data[\"image\"].to(device)\n",
    "    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
    "    targets = data[\"target_positions\"].to(device)\n",
    "    \n",
    "    outputs = model(inputs).reshape(targets.shape)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # not all the output steps are valid, but we can filter them out from the loss using availabilities\n",
    "    loss = loss * target_availabilities\n",
    "    loss = loss.mean()\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses_train.append(loss.item())\n",
    "        \n",
    "    progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save full trained model\n",
    "# torch.save(model.state_dict(), f'model_state_last.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning, you're running with a custom agents_mask\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7fa573f64a20>\n"
     ]
    }
   ],
   "source": [
    "# test configuration\n",
    "test_cfg = inference_cfg[\"test_data_loader\"]\n",
    "\n",
    "# Rasterizer\n",
    "rasterizer = build_rasterizer(inference_cfg, dm)\n",
    "\n",
    "# Test dataset/dataloader\n",
    "test_zarr = ChunkedDataset(dm.require(test_cfg[\"key\"])).open()\n",
    "test_mask = np.load(f\"{DIR_INPUT}/scenes/mask.npz\")[\"arr_0\"]\n",
    "test_dataset = AgentDataset(inference_cfg, test_zarr, rasterizer, agents_mask=test_mask)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             shuffle=test_cfg[\"shuffle\"],\n",
    "                             batch_size=test_cfg[\"batch_size\"],\n",
    "                             num_workers=test_cfg[\"num_workers\"])\n",
    "\n",
    "\n",
    "print(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saved state dict from the training notebook\n",
    "WEIGHT_FILE = 'model_state_last.pth'\n",
    "model_state = torch.load(WEIGHT_FILE, map_location=device)\n",
    "model.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device # cuba:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 4446/4446 [4:51:34<00:00,  2.72s/it]  \n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# store information for evaluation\n",
    "future_coords_offsets_pd = []\n",
    "timestamps = []\n",
    "\n",
    "agent_ids = []\n",
    "progress_bar = tqdm(test_dataloader)\n",
    "for data in progress_bar:\n",
    "    \n",
    "    inputs = data[\"image\"].to(device)\n",
    "    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
    "    targets = data[\"target_positions\"].to(device)\n",
    "\n",
    "    outputs = model(inputs).reshape(targets.shape)\n",
    "    \n",
    "    future_coords_offsets_pd.append(outputs.cpu().numpy().copy())\n",
    "    timestamps.append(data[\"timestamp\"].numpy().copy())\n",
    "    agent_ids.append(data[\"track_id\"].numpy().copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_pred_csv('xx.csv', # name your own submission file\n",
    "               timestamps=np.concatenate(timestamps),\n",
    "               track_ids=np.concatenate(agent_ids),\n",
    "               coords=np.concatenate(future_coords_offsets_pd),\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
